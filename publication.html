<!DOCTYPE html>
<html lang="en">
<script>
	(function (i, s, o, g, r, a, m) {
		i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
			(i[r].q = i[r].q || []).push(arguments)
		}, i[r].l = 1 * new Date(); a = s.createElement(o),
			m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
	})(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

	ga('create', 'UA-37298602-1', 'auto');
	ga('send', 'pageview');

</script>

<head>
	<title>Nguyen D. Vo</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="author" content="owwwlab.com">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<meta name="description" content="Nguyen D. Vo" />
	<meta name="keywords" content="faculty profile, theme,css, html, jquery, transition, transform, 3d, css3" />

	<link rel="shortcut icon" href="../favicon.ico">

	<!--CSS styles-->
	<link rel="stylesheet" href="css/bootstrap.css">
	<link rel="stylesheet" href="css/font-awesome.css">
	<link rel="stylesheet" href="css/perfect-scrollbar-0.4.5.min.css">
	<link rel="stylesheet" href="css/magnific-popup.css">
	<link rel="stylesheet" href="css/style.css">
	<link id="theme-style" rel="stylesheet" href="css/styles/default.css">


	<!--/CSS styles-->
	<!--Javascript files-->
	<script type="text/javascript" src="js/jquery-1.11.3.min.js"></script>
	<script type="text/javascript" src="js/TweenMax.min.js"></script>
	<script type="text/javascript" src="js/jquery.touchSwipe.min.js"></script>
	<script type="text/javascript" src="js/jquery.carouFredSel-6.2.1-packed.js"></script>

	<script type="text/javascript" src="js/modernizr.custom.63321.js"></script>
	<script type="text/javascript" src="js/jquery.dropdownit.js"></script>

	<script type="text/javascript" src="js/ScrollToPlugin.min.js"></script>

	<script type="text/javascript" src="js/bootstrap.min.js"></script>

	<script type="text/javascript" src="js/jquery.mixitup.min.js"></script>

	<script type="text/javascript" src="js/masonry.min.js"></script>

	<script type="text/javascript" src="js/perfect-scrollbar-0.4.5.with-mousewheel.min.js"></script>
	<script type="text/javascript" src="js/jquery.nicescroll.min.js"></script>

	<script type="text/javascript" src="js/magnific-popup.js"></script>
	<script type="text/javascript" src="js/custom.js"></script>

	<!--/Javascript files-->

</head>

<body>

	<div id="wrapper">
		<a href="#sidebar" class="mobilemenu">
			<i class="fa fa-reorder"></i>
		</a>

		<div id="sidebar">
			<div id="sidebar-wrapper">
				<div id="sidebar-inner">
					<!-- Profile/logo section-->
					<div id="profile" class="clearfix">
						<div class="portrate hidden-xs">
							<img class="logo-uit" src="img/LogoUIT.png" alt="" style="width:100%;height:100%;">							
						</div>
						<div class="title">
							<h2>Nguyen D. VO</h2>
							<h3>University of Information Technology</h3>
							<h3>VNUHCM, VietNam</h3>
						</div>
					</div>
					<!-- /Profile/logo section-->

					<!-- Main navigation-->
					<div id="main-nav">
						<ul id="navigation">
							<li>
								<a href="index.html">
									<i class="fa fa-home"></i>
									<div class="text">Home</div>
								</a>
							</li>
							<!-- <li>
								<a href="bio.html">
									<i class="fa fa-user"></i>
									<div class="text">About Me</div>
								</a>
							</li> -->

							<li class="currentmenu">
								<a href="publication.html">
									<i class="fa fa-edit"></i>
									<div class="text">Publications</div>
								</a>
							</li>

							<!-- <li>
								<a href="contact.html">
									<i class="fa fa-calendar"></i>
									<div class="text">Contact Me</div>
								</a>
							</li> -->


						</ul>
					</div>
					<!-- /Main navigation-->
					<!-- Sidebar footer -->
					<div id="sidebar-footer">
						<!-- <div class="social-icons">
							<ul>
								<li>
									<a href="https://www.linkedin.com/in/thanh-dat-truong-098544144/">
										<i class="fa fa-linkedin"></i>
									</a>
								</li>
							</ul>
						</div> -->


					</div>
					<!-- /Sidebar footer -->
				</div>

			</div>
		</div>

		<div id="main">

			<div id="publications" class="page">
				<div class="page-container">
					<div class="pageheader">
						<div class="headercontent">
							<div class="section-container">
								<h2 class="title">  </h2>
								<!-- <h2 class="title">Publications - 
								<a href="https://scholar.google.com/citations?user=LMLI9ZsAAAAJ&hl=en">Google Scholar</a></h2> -->
							</div>
						</div>
					</div>

					<div class="pagecontents">
						<!--
                            <div class="section color-1" id="filters"> <div class="section-container"> <div class="row">

                                        <div class="col-md-3"> <h3>Filter by type:</h3> </div> <div class="col-md-6"> <select id="cd-dropdown" name="cd-dropdown" class="cd-select"> 
                                        <option class="filter" value="all" selected>All types</option> 
                                        <option class="filter" value="jpaper">Journal Papers</option> 
                                        <option class="filter" value="cpaper">Conference Papers</option> 
                                        <option class="filter" value="wpaper">Workshop/Short Papers</option>
										<option class="filter" value="workingpapers">Working Papers</option>
                                        <!--<option class="filter" value="bookchapter">Book Chapters</option> <option class="filter" value="book">Books</option> <option class="filter" value="report">Reports</option>
                                        <option class="filter" value="tpaper">Technical Papers</option> -->
						</select>
					</div>
					<!--
                                        <div class="col-md-3" id="sort"> <span>Sort by year:</span> <div class="btn-group pull-right">

                                                <button type="button" data-sort="data-year" data-order="desc" class="sort btn btn-default"><i class="fa fa-sort-numeric-asc"></i></button> <button type="button" data-sort="data-year" data-order="asc" class="sort btn btn-default"><i class="fa
                                                fa-sort-numeric-desc"></i></button> </div> </div> </div> </div> </div>
-->
					<div class="section color-2" id="pub-grid">
						<div class="section-container">

							<div class="row">
								<div class="col-md-12">
									<div class="pitems">
										<h2 class="title">Publications - 
											<a href="https://scholar.google.com/citations?user=LMLI9ZsAAAAJ&hl=en">Google Scholar</a>
										</h2>
										<h2>2022</h2>

										<div class="item mix cpaper" data-year="2022">
											<!-- <div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Analysis of Fog density on oriented object detection in aerial images</h4>
												<div class="pubauthor">		
													<strong>Nguyen D. Vo</strong>, Phuc Nguyen, Thang Truong, Luu Ngo, Kiet Huynh, Dung Dinh, Khang Nguyen															
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													2022 RIVF International Conference on Computing and Communication Technologies (RIVF 2022)
												</div>
											</div> -->
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Object detection in aerial images having many practical applications in real life is becoming popular along with the surging development of deep learning and UAVs (Unmanned Aerial Vehicles). However, adverse weather conditions such as rain, night, and fog might reduce the quality of input images and significantly affect the performance of many perfectly trained detectors (detectors are trained in clear weather conditions). Moreover, object detection in aerial images often seeks high accuracy, while well-known object detection methods use horizontal bounding boxes to represent the object’s location. These issues raise the inconsistency between classification and bounding box regression. Understanding the need for practical solutions, we create two experimental Fog datasets based on the original DOTA dataset to provide the indepth analysis of fog density on multiple well-established oriented object detectors, namely Gliding Vertex, R3Det and ReDet. Furthermore, our training resources achieve promising results, flexibly ensemble to other methods to enhance models’ performance and adapt to many adverse weather conditions.
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2022">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Empirical Study of Real-time One-Stage Object Detection Methods on Recyclable Waste Dataset</h4>
												<div class="pubauthor">		
													<strong>Nguyen D. Vo</strong>, Bao N. Tran, Huyen Ngoc N. Van, Khanh B. T. Duong, Thinh V. Le, Nghia Nguyen															
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													2022 RIVF International Conference on Computing and Communication Technologies (RIVF 2022)
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													One-stage object detection methods have proven their advantage in terms of both speed and accuracy, bringing the enormous potential for addressing vision tasks in real-time scenarios. One of them is recyclable waste detection, which has become a prevalent topic of interest during the COVID19 pandemic. Previous research on this subject has faced many obstacles, mainly due to the requirement of detecting highly deformable and often translucent objects in cluttered scenes without the context information usually presents in humancentric datasets. In this paper, we aim to explore the performance of state-of-the-art one-stage object detectors on ZeroWaste dataset, the first in-the-wild industrial-grade waste detection benchmark collected at a real Materials Recovery Facility (MRF). Our experiments have shown that recent one-stage detectors can obtain very competitive results, and in particular, YOLOv7 is the current best performer at 33.2% mAP on the ZeroWaste benchmark, to the best of our knowledge.
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2022">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Anomaly Analysis in Images and Videos: A Comprehensive Review</h4>
												<div class="pubauthor">				
													Tung Minh Tran, Tu N Vu, <strong>Nguyen D. Vo</strong>, Tam V Nguyen, Khang Nguyen																																			
												</div>
												<div class="pubcite">
													<span class="label label-success">Journal Paper</span>
													ACM Computing Surveys (CSUR)
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Anomaly analysis is an important component of any surveillance system. In recent years, it has drawn the attention of the computer vision and machine learning communities. In this article, our overarching goal is thus to provide a coherent and systematic review of state-of-the-art techniques and a comprehensive review of the research works in anomaly analysis. We would like to provide a broad vision of computational models, datasets, metrics, extensive experiments, and what anomaly analysis can do in images and videos. Intensively covering nearly 200 publications we review i) anomaly related surveys, ii) taxonomy for anomaly problems, iii) the computational models, iv) the benchmark datasets for studying abnormalities in images and videos, and v) the performance of state-of-the-art methods in this research problem. In addition, we provide insightful discussions and pave way to the future work.
												</p>
											</div>
										</div>

										



										<!-- <div class="item mix cpaper" data-year="2022">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">EAES: Effective Augmented Embedding Spaces for Text-Based Image Captioning</h4>
												<div class="pubauthor">																										
													Khang Nguyen, Doanh C Bui, Truc Trinh, <strong>Nguyen D. Vo</strong>
												</div>
												<div class="pubcite">
													<span class="label label-success">Journal Paper</span>
													IEEE Access 10, 32443-32452 
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
												Text-based Image Captioning has been a novel problem since 2020. This topic remains challenging because it requires the model to comprehend not only the visual context but also the scene texts that appear in an image. Therefore, the ways image and scene texts are embedded into the main model for training is crucial. Based on the M4C-Captioner model, this paper proposes the simple but effective EAES embedding module for effectively embedding images and scene texts into the multimodal Transformer layers. In detail, our EAES module contains two significant sub-modules: Objects-augmented and Grid features augmentation. With the Objects-augmented module, we provide the relative geometry feature, representing the relation between objects and between OCR tokens. Furthermore, we extract the grid features for an image with the Grid features augmentation module and combine it with visual objects, which help the model focus on both salient objects and the general context of an image, leading to better performance. We use the TextCaps dataset as the benchmark to prove the effectiveness of our approach on five standard metrics: BLEU4, METEOR, ROUGE-L, SPICE and CIDEr. Without bells and whistles, our method achieves 20.21% on the BLEU4 metric and 85.78% on the CIDEr metric, 1.31% and 4.78% higher, respectively, than the baseline M4C-Captioner method. Furthermore, the results are incredibly competitive with other methods on METEOR, ROUGE-L and SPICE metrics. Source code is available at https://github.com/UIT-Together/EAES_m4c .	
												</p>
											</div>
										</div> -->

										<!-- <div class="item mix cpaper" data-year="2022">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">CDeRSNet: Towards High Performance Object Detection in Vietnamese Document Images</h4>
												<div class="pubauthor">													
													Thuan Trong Nguyen, Thuan Q. Nguyen, Long Duong, <strong>Nguyen D. Vo</strong>, Khang Nguyen																							
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													28th International Conference on MultiMedia Modeling (MMM), 2022.												 
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													In recent years, document image understanding (DIU) has received much attention from the research community. Localizing page objects (tables, figures, equations) in document images is an important problem in DIU, which is the foundation for extracting information from document images. However, it has remained many challenges due to the high degree of intra-class variability in page document. Especially, object detection in Vietnamese image documents has still limited. In this paper, we propose CDeRSNet: a novel end-to-end trainable deep learning network to solve object detection in Vietnamese documents. The proposed network consists of Cascade R-CNN with the deformable convolution backbone and Rank & Sort (RS) Loss. CDeRSNet detects objects varying in scale with high detection accuracy at a higher IoU threshold to localize objects that differ in scale with detection accuracy at high quality. We empirically evaluate CDeRSNet on the Vietnamese image document dataset - UIT-DODV with four classes of objects: table, figure, caption, and formula. We achieved the best performance on the UIT-DODV dataset with 79.9% in terms of mAP, which is higher 5.4% than current results. In addition, we also provide a comprehensive evaluation and insightful analysis of CDeRSNet. Finally, we demonstrate CDeRSNet outperformance over state-of-the-arts models in object detection such as GFocal, GFocalV2, VFNet, DetectoRS on the UIT-DODV dataset. Code can be available at: https://github.com/trongthuan205/CDeRSNet.git.
												</p>
											</div>
										</div> -->


										<h2>2021</h2>
										<div class="item mix cpaper" data-year="2021">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Parsing digitized Vietnamese paper documents</h4>
												<div class="pubauthor">													
													Linh Truong Dieu, Thuan Trong Nguyen, <strong>Nguyen D. Vo</strong>, Tam V Nguyen, Khang Nguyen													
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													International Conference on Computer Analysis of Images and Patterns (CAIP), 2021.												 
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													In recent years, the need to exploit digitized document data has been increasing. In this paper, we address the problem of parsing digitized Vietnamese paper documents. The digitized Vietnamese documents are mainly in the form of scanned images with diverse layouts and special characters introducing many challenges. To this end, we first collect the UIT-DODV dataset, a novel Vietnamese document image dataset that includes scientific papers in Vietnamese derived from different scientific conferences. We compile both images that were converted from PDF and scanned by a smartphone in addition a physical scanner that poses many new challenges. Additionally, we further leverage the state-of-the-art object detector along with the fused loss function to efficiently parse the Vietnamese paper documents. Extensive experiments conducted on the UIT-DODV dataset provide a comprehensive evaluation and insightful analysis.
												</p>
											</div>
										</div>


										<h2>2020</h2>
										<div class="item mix cpaper" data-year="2020">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Detecting objects from space: An evaluation of deep-learning modern approaches</h4>
												<div class="pubauthor">													
													Khang Nguyen, Nhut T Huynh, Phat C Nguyen, Khanh-Duy Nguyen, <strong>Nguyen D. Vo</strong>, Tam V Nguyen
												</div>
												<div class="pubcite">
													<span class="label label-success">Journal Paper</span>
													Electronics 2020, 9(4), 583													 
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Unmanned aircraft systems or drones enable us to record or capture many scenes from the bird’s-eye view and they have been fast deployed to a wide range of practical domains, i.e., agriculture, aerial photography, fast delivery and surveillance. Object detection task is one of the core steps in understanding videos collected from the drones. However, this task is very challenging due to the unconstrained viewpoints and low resolution of captured videos. While deep-learning modern object detectors have recently achieved great success in general benchmarks, i.e., PASCAL-VOC and MS-COCO, the robustness of these detectors on aerial images captured by drones is not well studied. In this paper, we present an evaluation of state-of-the-art deep-learning detectors including Faster R-CNN (Faster Regional CNN), RFCN (Region-based Fully Convolutional Networks), SNIPER (Scale Normalization for Image Pyramids with Efficient Resampling), Single-Shot Detector (SSD), YOLO (You Only Look Once), RetinaNet, and CenterNet for the object detection in videos captured by drones. We conduct experiments on VisDrone2019 dataset which contains 96 videos with 39,988 annotated frames and provide insights into efficient object detectors for aerial images.
												</p>
											</div>
										</div>

										<h2>2019</h2>
										<div class="item mix cpaper" data-year="2019">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Phát hiện phương tiện giao thông trong không ảnh</h4>
												<div class="pubauthor">
													Đỗ Trung Hiếu, Phan Đức Anh, <strong>Võ Duy Nguyên</strong>, Nguyễn Tấn Trần Minh Khang
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													Hội thảo khoa học quốc gia Công nghệ thông tin & Ứng dụng trong các lĩnh vực (CITA), 2019.													 
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Thiết bị bay không người lái ngày càng có nhiều ứng dụng trong đờisống. Việc phát hiện phương tiện giao thông cũng trở thành nhiệm vụ cho các thiết bị bay giám sát giao thông. Đối tượng trong không ảnh nhỏ hơn rất nhiều so với ảnh chụp từ camera mặt đất, đây là thách thức rất lớn. Với các đối tượng nhỏ, sự sai khác của các vùng đề xuất sẽ làm ảnh hưởng lớn đến kết quả phát hiện đối tượng. Trong nghiên cứu này, chúng tôi sử dụng Faster R-CNN, một phương pháp tiên tiến về phát hiện đối tượng, đánh giá thực nghiệm với hai hiệu chỉnh vùng đề xuất là RoIPool và RoiAlign. Thực nghiệm cho thấy RoIAlign cho kết quả tốt hơn RoIPool ở đa số các lớp đối tượng, đặc biệt là các đối tượng nhỏ. Thông qua thực nghiệm, chúng tôi đưa ra kết quả, nhận xét tìm được làm cơ sở cho các nghiên cứu sau.
												</p>
											</div>
										</div>


										<h2>2018</h2>										
										<div class="item mix cpaper" data-year="2018">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">Ensemble of deep object detectors for page object detection</h4>
												<div class="pubauthor">
													<strong>Nguyen D. Vo</strong>, Khanh Nguyen, Tam V Nguyen, Khang Nguyen
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													International Conference on Ubiquitous Information Management and Communication (IMCOM'18), 2018.													 
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Document Imaging Understanding (DIU) is the process of converting all of the information content of a document image digital into an electronic format launched its reasonable content. We first evaluate different state-of-the-art object detection methods (Fast R-CNN and Faster R-CNN) for the task on POD dataset. We observe that each detection method is sensitive to certain objects. Therefore, we propose combining the detections of Fast RCNN and Faster RCNN in order to exploit the advantages of the two models. Through the extensive experiments, our proposed method significantly improves from 81.35%(Faster R-CNN VGG_CNN_M_1024) to 86.49% in terms of the mAP.	
												</p>
											</div>
										</div>

										<!-- <div class="item mix cpaper" data-year="2022">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">OTAdapt: Optimal Transport-based Approach For Unsupervised Domain Adaptation</h4>
												<div class="pubauthor">
													<strong>Nguyen D. Vo</strong>, Naga Venkata Sai Raviteja Chappa, Xuan Bac Nguyen, Ngan Le, Ashley Dowling, and
													Khoa Luu.
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													The International Conference on Pattern Recognition (ICPR), 2022.
													 
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Unsupervised domain adaptation is one of the challenging problems in computer vision.
													This paper presents a novel approach to unsupervised domain adaptations based on the optimal transport-based distance. 
													Our approach allows aligning target and source domains without the requirement of meaningful metrics across domains. 
													In addition, the proposal can associate the correct mapping between source and target domains and guarantee a constraint 
													of topology between source and target domains.
													The proposed method is evaluated on different datasets in various problems, 
													i.e. (i) digit recognition on MNIST, MNIST-M, USPS datasets, 
													(ii) Object recognition on Amazon, Webcam, DSLR, and VisDA datasets, 
													(iii) Insect Recognition on the IP102 dataset.
													The experimental results show that our proposed method consistently improves performance accuracy. 
													Also, our framework could be incorporated with any other CNN frameworks within an end-to-end deep network design for recognition problems to improve their performance.
												</p>
											</div>
										</div>

										<div class="item mix cpaper" data-year="2022">
											<div class="pubmain">
												<div class="pubassets">
													<a href="#" class="pubcollapse">
														<i class="fa fa-expand"></i>
													</a>
												</div>
												<h4 class="pubtitle">DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition</h4>
												<div class="pubauthor">
													<strong>Nguyen D. Vo</strong>, Quoc-Huy Bui, Chi Nhan Duong, Han-Seok Seo, Son Lam Phung, Xin Li, 
													and Khoa Luu
												</div>
												<div class="pubcite">
													<span class="label label-success">Conference Paper</span>
													The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2022
													 
												</div>
											</div>
											<div class="pubdetails">
												<h4>Abstract</h4>
												<p>
													Human action recognition has recently become one of the popular research topics in the computer vision community. 
													Various 3D-CNN based methods have been presented to tackle both the spatial and temporal dimensions in the task 
													of video action recognition with competitive results. However, these methods have suffered some fundamental 
													limitations such as lack of robustness and generalization, e.g., how does the temporal ordering of video frames affect the recognition results?
													This work presents a novel end-to-end Transformer-based Directed Attention (DirecFormer) framework for robust action recognition. 
													The method takes a simple but novel perspective of Transformer-based approach to understand the right order of sequence actions.  
													Therefore, the contributions of this work are three-fold. Firstly, we introduce the problem of ordered temporal learning issues to the action recognition problem. 
													Secondly, a new Directed Attention mechanism is introduced to understand and provide attentions to human actions in the right order. 
													Thirdly, we introduce the conditional dependency in action sequence modeling that includes orders and classes. 
													The proposed approach consistently achieves the state-of-the-art (SOTA) results compared with the recent action recognition methods, 
													on three standard large-scale benchmarks, i.e. Jester, Kinetics-400 and Something-Something-V2.
												</p>
											</div>
										</div> -->







									</div>
								</div>
							</div>
						</div>

					</div>
				</div>

			</div>
		</div>
	</div>


	</div>
	</div>
</body>

</html>